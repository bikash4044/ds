{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds a penalty to the absolute values of the coefficients. This penalty can shrink some coefficients to zero, effectively selecting features and reducing complexity. \n",
    "\n",
    "It differs from other regression techniques, like Ridge Regression, which penalizes the sum of squared coefficients and does not shrink coefficients to zero, thus retaining all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Ans:\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is its ability to shrink some coefficients to zero, effectively removing less important features and simplifying the model. This helps in identifying the most relevant features and avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Ans:\n",
    "\n",
    "In a Lasso Regression model, coefficients represent the strength and direction of the relationship between each feature and the target variable. A zero coefficient means the feature is not important in predicting the target, as Lasso can set some coefficients to zero during feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "Ans:\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter, often denoted as lambda or alpha. This parameter controls the strength of the penalty applied to the coefficients:\n",
    "\n",
    "High lambda: Increases the penalty, which can lead to more coefficients being shrunk to zero, simplifying the model but potentially underfitting.\n",
    "Low lambda: Decreases the penalty, allowing more features to remain in the model, which can lead to overfitting if too low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Yes, Lasso Regression can be used for non-linear regression problems by transforming the input features into a higher-dimensional space. This can be done using techniques like polynomial features, where new features are created by combining the original features to capture non-linear relationships. After this transformation, Lasso can be applied to select important features and fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "- Adds a penalty based on the sum of the squared coefficients.\n",
    "- Formula: RSS + lambda * sum(beta_i^2)\n",
    "- Shrinks coefficients but doesn't set them to zero.\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "- Adds a penalty based on the sum of the absolute values of the coefficients.\n",
    "- Formula: RSS + lambda * sum(|beta_i|)\n",
    "- Can shrink some coefficients to zero, which helps in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent. It does this by:\n",
    "\n",
    "Feature Selection: Lasso can set some coefficients exactly to zero, effectively removing less important features from the model. This reduces redundancy among correlated features.\n",
    "\n",
    "Shrinking Coefficients: By applying the L1 penalty, Lasso reduces the impact of less significant features, which can help mitigate the effects of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "Ans:\n",
    "\n",
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, we can use the following methods:\n",
    "\n",
    "Cross-Validation: Split the data into training and validation sets. Train the Lasso model on the training set with different lambda values and evaluate performance on the validation set. The lambda that results in the best performance (e.g., lowest mean squared error) is selected.\n",
    "\n",
    "Grid Search: Systematically test a range of lambda values. Combine this with cross-validation to find the lambda that performs best on average.\n",
    "\n",
    "Regularization Path Algorithms: Use algorithms like LARS (Least Angle Regression) that compute solutions for a range of lambda values efficiently. This approach provides a path of solutions and helps in choosing the optimal lambda.\n",
    "\n",
    "Information Criteria: Use criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) that balance model fit with complexity to select lambda."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
