{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Assumptions of ANOVA\n",
    "\n",
    "Independence of Observations:\n",
    "\n",
    "Assumption: The observations within each group and between groups are independent of each other. This means that the value of one observation does not influence or is not related to the value of another observation.\n",
    "\n",
    "Violation Example: If participants in a study are measured multiple times or if there is a clustering effect (e.g., students within the same classroom), the independence assumption may be violated.\n",
    "Normality:\n",
    "\n",
    "Assumption: The data in each group should be approximately normally distributed. ANOVA is robust to minor deviations from normality, especially with larger sample sizes, but severe deviations can affect the results.\n",
    "\n",
    "Violation Example: If the data are highly skewed or have heavy tails, the assumption of normality is violated. For instance, if the reaction times in a study have extreme outliers, the normality assumption may be compromised.\n",
    "Homogeneity of Variances (Homoscedasticity):\n",
    "\n",
    "Assumption: The variances among the groups should be approximately equal. This means that the spread or dispersion of data points around the mean is similar across all groups.\n",
    "\n",
    "Violation Example: If one group has a much larger variance than the others (e.g., one treatment group shows much more variability in response compared to others), the assumption of homogeneity of variances is violated. This can be tested using Levene's test or Bartlett's test.\n",
    "\n",
    "\n",
    "Examples of Violations and Their Impact\n",
    "\n",
    "Independence of Observations:\n",
    "\n",
    "Example: In a clinical trial, if patients are treated in groups and their responses are correlated due to the same environment or treatment, the independence assumption is violated. This can lead to inflated Type I error rates or reduced power of the test.\n",
    "Normality:\n",
    "\n",
    "Example: If test scores are not normally distributed but are instead heavily skewed (e.g., many students score very low or very high), the results of ANOVA might be misleading. Transformations (e.g., log or square root) or non-parametric alternatives (e.g., Kruskal-Wallis test) might be needed.\n",
    "Homogeneity of Variances:\n",
    "\n",
    "Example: In an educational study comparing test scores among different teaching methods, if one method produces very diverse outcomes while others are more consistent, the assumption of equal variances is violated. This can lead to inaccurate F-statistic calculations and potentially incorrect conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "\n",
    "Ans:\n",
    "\n",
    "1. One-Way ANOVA\n",
    "Purpose: Tests for differences in the means of three or more independent groups based on one factor.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "Single Factor: When you have one independent variable (factor) with three or more levels (groups).\n",
    "Example: Comparing the effectiveness of three different diets (low-carb, low-fat, Mediterranean) on weight loss.\n",
    "\n",
    "\n",
    "2. Two-Way ANOVA\n",
    "Purpose: Tests for differences in the means of groups based on two independent factors and evaluates the interaction between these factors.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "Two Factors: When you have two independent variables and want to understand their individual effects and the interaction effect on a dependent variable.\n",
    "Example: Studying the effect of diet type (low-carb vs. low-fat) and exercise level (high vs. low) on weight loss.\n",
    "\n",
    "\n",
    "3. Repeated Measures ANOVA\n",
    "Purpose: Tests for differences in the means of three or more related groups (i.e., measurements taken on the same subjects at different times or under different conditions).\n",
    "\n",
    "When to Use:\n",
    "\n",
    "Related Groups: When the same subjects are measured multiple times or under different conditions.\n",
    "Example: Measuring the blood pressure of patients before, during, and after a treatment to assess how treatment affects blood pressure over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Partitioning of Variance\n",
    "Total Variance: The overall variability in the data.\n",
    "\n",
    "Between-Group Variance: Measures how much the means of different groups differ from each other. It shows the effect of the different treatments or conditions.\n",
    "\n",
    "Within-Group Variance: Measures the variability within each group. It shows how much individual observations in the same group differ from their group's mean.\n",
    "\n",
    "Importance\n",
    "Identifying Sources of Variability: Helps determine how much of the total variability is due to differences between groups versus differences within groups.\n",
    "\n",
    "Assessing Factors' Effectiveness: Shows whether the differences between group means are significant compared to the variability within groups.\n",
    "\n",
    "Interpreting Results: The F-ratio, which compares between-group variance to within-group variance, helps to decide if the group means are significantly different.\n",
    "\n",
    "Guiding Further Analysis: Helps in planning additional tests if significant differences are found, to find out which specific groups differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Total Sum of Squares (SST): Measures the total variability in the data.\n",
    "Explained Sum of Squares (SSE): Measures the variability explained by the group differences.\n",
    "Residual Sum of Squares (SSR): Measures the variability within groups, not explained by the group differences.\n",
    "\n",
    "Explained with a sample code below in next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 410.93333333333334\n",
      "Explained Sum of Squares (SSE): 376.1333333333333\n",
      "Residual Sum of Squares (SSR): 34.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Group': ['A']*5 + ['B']*5 + ['C']*5,\n",
    "    'Value': [23, 20, 22, 24, 25, 30, 29, 31, 32, 28, 35, 34, 33, 36, 37]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = df.groupby('Group')['Value'].mean()\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = df['Value'].mean()\n",
    "\n",
    "# Calculate SST (Total Sum of Squares)\n",
    "SST = np.sum((df['Value'] - overall_mean) ** 2)\n",
    "\n",
    "# Calculate SSE (Explained Sum of Squares)\n",
    "SSE = np.sum(df.groupby('Group').size() * (group_means - overall_mean) ** 2)\n",
    "\n",
    "# Calculate SSR (Residual Sum of Squares)\n",
    "SSR = np.sum((df['Value'] - df.groupby('Group')['Value'].transform('mean')) ** 2)\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Main Effects:\n",
    "\n",
    "Main Effect of Factor A: Measures how the levels of Factor A (e.g., different teaching methods) impact the dependent variable. It is calculated by comparing the average outcomes across the levels of Factor A, averaging over all levels of Factor B.\n",
    "Main Effect of Factor B: Measures how the levels of Factor B (e.g., different study times) impact the dependent variable. It is calculated by comparing the average outcomes across the levels of Factor B, averaging over all levels of Factor A.\n",
    "\n",
    "Interaction Effect:\n",
    "\n",
    "Interaction Effect of Factors A and B: Measures how the effect of one factor (e.g., teaching method) changes depending on the level of the other factor (e.g., study time). It is calculated by examining whether the differences in the outcomes between levels of Factor A are consistent across the levels of Factor B and vice versa.\n",
    "To calculate these effects, you use the sum of squares for each factor and their interaction, which can be obtained through an ANOVA table. The main effects and interaction effects are assessed by comparing their corresponding mean squares to the residual mean square (error variance) to determine statistical significance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Statistical Significance:\n",
    "\n",
    "P-Value: The p-value of 0.02 is less than the common significance level of 0.05. This indicates that there is a statistically significant difference between the means of at least two of the groups.\n",
    "Conclusion: Since the p-value is below 0.05, you reject the null hypothesis, which states that all group means are equal. This suggests that at least one group mean is significantly different from the others.\n",
    "F-Statistic:\n",
    "\n",
    "F-Statistic: The F-statistic of 5.23 quantifies how much the group means differ from each other relative to the variability within the groups. A higher F-statistic generally indicates a larger difference between group means compared to within-group variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Handling Missing Data\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion)\n",
    "\n",
    "Description: Only include subjects with complete data for all measurements. Any subject with missing data is excluded from the analysis.\n",
    "Consequences:\n",
    "Pros: Simple and straightforward; maintains the integrity of the dataset for analysis.\n",
    "Cons: Can lead to reduced sample size and loss of statistical power. May introduce bias if the missing data is not missing completely at random (MCAR).\n",
    "\n",
    "Pairwise Deletion\n",
    "\n",
    "Description: Use all available data for each analysis. For each pair of variables, use only the cases with non-missing values for those variables.\n",
    "Consequences:\n",
    "Pros: Utilizes more of the available data than complete case analysis, potentially preserving statistical power.\n",
    "Cons: Results can vary depending on the amount and pattern of missing data. May complicate interpretation and affect the consistency of results.\n",
    "\n",
    "Imputation Methods\n",
    "\n",
    "Description: Replace missing data with estimated values based on other available data. Common methods include mean imputation, regression imputation, and multiple imputation.\n",
    "Consequences:\n",
    "Pros: Can provide a complete dataset and preserve sample size, improving statistical power. Multiple imputation accounts for uncertainty by creating several imputed datasets.\n",
    "Cons: Imputation can introduce its own biases and inaccuracies. Simple imputation methods (e.g., mean imputation) may underestimate variability and reduce the reliability of statistical tests.\n",
    "\n",
    "Model-Based Approaches\n",
    "\n",
    "Description: Use statistical models that handle missing data explicitly, such as maximum likelihood estimation or Bayesian methods.\n",
    "Consequences:\n",
    "Pros: Often provide robust results and incorporate the uncertainty of missing data. Methods like maximum likelihood can handle various missing data mechanisms.\n",
    "Cons: More complex to implement and interpret. Requires careful model specification and assumptions.\n",
    "Potential Consequences of Different Methods\n",
    "Bias: Depending on the missing data mechanism (Missing Completely at Random, Missing at Random, Missing Not at Random), different methods may introduce bias. For instance, listwise deletion may introduce bias if the missing data is related to the outcome.\n",
    "\n",
    "Statistical Power: Methods that reduce sample size, such as listwise deletion, can lower the power of statistical tests and increase Type II errors. Imputation and model-based approaches can help retain sample size and power.\n",
    "\n",
    "Validity of Results: Imputation methods and model-based approaches can help maintain the validity of results, provided they are applied correctly and the assumptions are met. Incorrect imputation or model specification can lead to misleading conclusions.\n",
    "\n",
    "Complexity: Advanced methods like multiple imputation or model-based approaches are more complex to implement and require careful consideration of assumptions and data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Tukey’s Honestly Significant Difference (HSD) Test\n",
    "\n",
    "When to Use: When you want to compare all possible pairs of group means and control for Type I error rate. It is used when the assumption of equal variances (homogeneity of variances) is met.\n",
    "Example: After finding significant differences among three diets (low-carb, low-fat, Mediterranean) in weight loss, Tukey's HSD can identify which specific diet pairs (e.g., low-carb vs. Mediterranean) show significant differences.\n",
    "\n",
    "Bonferroni Correction\n",
    "\n",
    "When to Use: When you have multiple comparisons and want to control the family-wise error rate. It adjusts the significance level for each test to reduce the likelihood of Type I errors.\n",
    "Example: If you test the effects of four different teaching methods on student performance, Bonferroni correction helps control for the increased risk of false positives due to multiple comparisons.\n",
    "\n",
    "Scheffé’s Test\n",
    "\n",
    "When to Use: When you need a more conservative test for comparing multiple group means, particularly when dealing with unequal sample sizes or variances. It is useful for complex comparisons (e.g., comparing combinations of groups).\n",
    "Example: If you are comparing several different therapeutic interventions and their combinations on recovery rates, Scheffé’s test can handle complex comparisons and is less likely to produce false positives.\n",
    "\n",
    "Dunnett’s Test\n",
    "\n",
    "When to Use: When comparing multiple treatment groups to a single control group. It is specifically designed for comparing each treatment group against a control.\n",
    "Example: In a study evaluating the effectiveness of new medications compared to a standard treatment, Dunnett’s test helps determine which new medications are significantly different from the standard treatment.\n",
    "\n",
    "Newman-Keuls Test\n",
    "\n",
    "When to Use: When you want a less conservative test compared to Tukey’s HSD. It is sequential and more powerful but may increase the risk of Type I errors.\n",
    "Example: In a study with multiple treatment levels, Newman-Keuls test can identify differences between groups with greater sensitivity, but with a slightly higher risk of Type I errors.\n",
    "Example of Using a Post-Hoc Test\n",
    "Scenario: Suppose you conduct a one-way ANOVA to examine the effect of three different types of exercise (yoga, running, swimming) on stress levels. The ANOVA result is significant, indicating that there are differences in stress levels among the three exercise types.\n",
    "\n",
    "Need for Post-Hoc Test:\n",
    "\n",
    "Since the ANOVA only tells you that not all group means are equal but not which specific groups differ, you need a post-hoc test.\n",
    "Choice: If you want to compare each pair of exercise types while controlling for Type I error, you might use Tukey’s HSD.\n",
    "Action: Perform Tukey’s HSD to find out if the difference in stress levels between yoga and running is significant, or if yoga differs from swimming, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 2.794572222829868\n",
      "P-value: 0.07132490182725253\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "diet_A = np.random.normal(loc=5, scale=1, size=17)  # Sample data for Diet A\n",
    "diet_B = np.random.normal(loc=6, scale=1, size=16)  # Sample data for Diet B\n",
    "diet_C = np.random.normal(loc=5.5, scale=1, size=17)  # Sample data for Diet C\n",
    "\n",
    "F_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "F-Statistic: The F-statistic of 2.7946 represents the ratio of the variance between the group means to the variance within the groups. This value indicates how much the group means differ relative to the variability within each group.\n",
    "\n",
    "P-Value: The p-value of 0.0713 is greater than the common significance level of 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               sum_sq    df         F    PR(>F)\n",
      "C(Program)                   0.149574   2.0  0.002406  0.961057\n",
      "C(Experience)                     NaN   1.0       NaN       NaN\n",
      "C(Program):C(Experience)   216.995689   2.0  3.489854  0.066981\n",
      "Residual                  1741.012312  56.0       NaN       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bikas\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "c:\\Users\\bikas\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 1, but rank is 0\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "c:\\Users\\bikas\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1923: RuntimeWarning: invalid value encountered in divide\n",
      "  F /= J\n",
      "c:\\Users\\bikas\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "programs = ['A', 'B', 'C']\n",
    "experience_levels = ['Novice', 'Experienced']\n",
    "n_per_group = 10\n",
    "\n",
    "data = {\n",
    "    'Time': np.concatenate([\n",
    "        np.random.normal(loc=30, scale=5, size=n_per_group),\n",
    "        np.random.normal(loc=25, scale=5, size=n_per_group),\n",
    "        np.random.normal(loc=28, scale=5, size=n_per_group),\n",
    "        np.random.normal(loc=22, scale=5, size=n_per_group),\n",
    "        np.random.normal(loc=27, scale=5, size=n_per_group),\n",
    "        np.random.normal(loc=21, scale=5, size=n_per_group)\n",
    "    ]),\n",
    "    'Program': (['A']*n_per_group + ['A']*n_per_group +\n",
    "                ['B']*n_per_group + ['B']*n_per_group +\n",
    "                ['C']*n_per_group + ['C']*n_per_group),\n",
    "    'Experience': (['Novice']*n_per_group*3 + ['Experienced']*n_per_group*3)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = ols('Time ~ C(Program) * C(Experience)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "Effect of Program (C(Program)):\n",
    "\n",
    "Sum of Squares (sum_sq): 0.149574\n",
    "Degrees of Freedom (df): 2\n",
    "F-Statistic (F): 0.002406\n",
    "P-Value (PR(>F)): 0.961057\n",
    "Interpretation: The p-value for the effect of the software program is 0.961, which is much greater than the typical significance level of 0.05. This indicates that there are no significant differences in task completion times among the three software programs. The F-statistic is very low, which suggests that the variance between the means of different programs is negligible compared to the variance within the groups.\n",
    "\n",
    "Effect of Experience (C(Experience)):\n",
    "\n",
    "Sum of Squares (sum_sq): NaN\n",
    "Degrees of Freedom (df): 1\n",
    "F-Statistic (F): NaN\n",
    "P-Value (PR(>F)): NaN\n",
    "Interpretation: The values for C(Experience) are NaN, which typically occurs when there is no variation in the Experience variable across the groups, or if the Experience variable does not contribute any additional variation after accounting for the other factors. This indicates that the model might not be properly capturing the effect of experience, or there may be an issue with the data related to experience.\n",
    "\n",
    "Interaction Effect (C(Program)\n",
    "(Experience)):\n",
    "\n",
    "Sum of Squares (sum_sq): 216.995689\n",
    "Degrees of Freedom (df): 2\n",
    "F-Statistic (F): 3.489854\n",
    "P-Value (PR(>F)): 0.066981\n",
    "Interpretation: The p-value for the interaction effect is 0.067, which is slightly above the 0.05 significance level. This suggests that the interaction between software programs and experience levels is marginally significant. The F-statistic indicates that there is some evidence that the effect of software programs on task completion time may depend on the level of experience of the employees, but the effect is not statistically significant at the 5% level. It may be worth investigating further, especially if practical significance is of concern.\n",
    "\n",
    "Residual (Error) Terms:\n",
    "\n",
    "Sum of Squares (sum_sq): 1741.012312\n",
    "Degrees of Freedom (df): 56\n",
    "Interpretation: The residual sum of squares represents the variation not explained by the model. The degrees of freedom for residuals reflect the number of observations minus the number of parameters estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -0.6823599641573114\n",
      "P-value: 0.4966210864762014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(0)\n",
    "control_group_scores = np.random.normal(loc=75, scale=10, size=50)\n",
    "experimental_group_scores = np.random.normal(loc=78, scale=10, size=50)\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(control_group_scores, experimental_group_scores)\n",
    "\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "T-Statistic:\n",
    "\n",
    "The t-statistic of -0.6824 indicates the direction and magnitude of the difference between the means of the two groups. The negative sign suggests that the mean test score of the experimental group is lower than that of the control group, but this difference is relatively small.\n",
    "P-Value:\n",
    "\n",
    "The p-value of 0.4966 is much greater than the common significance level of 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store  0.9395 2.0000 58.0000 0.3967\n",
      "===================================\n",
      "\n",
      "P-value: 0.39667683341525484\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import pingouin as pg\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(0)\n",
    "days = 30\n",
    "sales_a = np.random.normal(loc=1000, scale=100, size=days)\n",
    "sales_b = np.random.normal(loc=1100, scale=100, size=days)\n",
    "sales_c = np.random.normal(loc=1050, scale=100, size=days)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Day': np.tile(np.arange(1, days + 1), 3),\n",
    "    'Store': ['A']*days + ['B']*days + ['C']*days,\n",
    "    'Sales': np.concatenate([sales_a, sales_b, sales_c])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "aovrm = AnovaRM(df, 'Sales', 'Day', within=['Store'])\n",
    "anova_results = aovrm.fit()\n",
    "\n",
    "# Print ANOVA results summary\n",
    "print(anova_results)\n",
    "\n",
    "# Extract p-value from ANOVA results\n",
    "p_value = anova_results.anova_table['Pr > F']['Store']\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# If significant, perform post-hoc test\n",
    "if p_value < 0.05:\n",
    "    posthoc = pg.pairwise_tukey(df, dv='Sales', between='Store')\n",
    "    print(posthoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "F-Statistic:\n",
    "\n",
    "The F-value of 0.9395 measures the ratio of variance between the store means to the variance within the stores. A lower F-value suggests that the between-group variance is small relative to the within-group variance.\n",
    "P-Value:\n",
    "\n",
    "The p-value of 0.3967 is much greater than the typical significance level of 0.05."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
