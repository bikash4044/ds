{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Simple Linear Regression\n",
    "\n",
    "Definition: Models the relationship between one independent variable and one dependent variable.\n",
    "\n",
    "Formula: Y = a + b*X \n",
    "\n",
    "Example: Predicting weight based on height.\n",
    "\n",
    "Multiple Linear Regression\n",
    "\n",
    "Definition: Models the relationship between one dependent variable and multiple independent variables.\n",
    "\n",
    "Formula: Y = a + b1X1 + b2X2 + ... + bn*Xn \n",
    "\n",
    "Example: Predicting weight based on height, age, and gender, combinely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Linearity:\n",
    "\n",
    "The relationship between the independent and dependent variables is linear, meaning changes in the independent variables lead to proportional changes in the dependent variable. This assumption ensures that the model accurately reflects the relationship as a straight line.\n",
    "\n",
    "Independence:\n",
    "\n",
    "The residuals (errors) must be independent of each other, meaning there is no correlation between the residuals of different observations. This ensures that each data point provides unique information and the model's predictions are reliable.\n",
    "\n",
    "Homoscedasticity:\n",
    "\n",
    "The variance of residuals should be constant across all levels of the independent variables. If residuals spread out more widely or more narrowly at different levels of the independent variables, it indicates heteroscedasticity, which can lead to inefficiencies in the model.\n",
    "\n",
    "Normality of Residuals:\n",
    "\n",
    "The residuals should be normally distributed to ensure valid hypothesis testing and confidence intervals. Normally distributed residuals allow for accurate statistical inference and predictions.\n",
    "\n",
    "No Multicollinearity:\n",
    "\n",
    "Independent variables should not be highly correlated with each other. High multicollinearity can lead to unstable estimates of regression coefficients and make it difficult to assess the effect of each independent variable.\n",
    "\n",
    "Methods to check:\n",
    "\n",
    "Linearity: Create scatter plots of independent variables against the dependent variable and plot residuals against predicted values.\n",
    "\n",
    "Independence: Use the Durbin-Watson test to detect autocorrelation in residuals.\n",
    "\n",
    "Homoscedasticity: Plot residuals against predicted values and look for constant spread.\n",
    "\n",
    "Normality of Residuals: Use a Q-Q plot or Shapiro-Wilk test to check for normality of residuals.\n",
    "\n",
    "No Multicollinearity: Calculate the Variance Inflation Factor (VIF) for each independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Slope represents the change in the depended variable due to unit change in independent variable.\n",
    "\n",
    "Intercept is the value of dependent variable when all the independent variable are 0.\n",
    "\n",
    "Example: Predicting a person’s weight based on height.\n",
    "\n",
    "Model: Weight = 50 + 2*Height\n",
    "\n",
    "Intercept (50): Represents the estimated weight of a person with a height of zero. Though not practical, it serves as a baseline.\n",
    "\n",
    "Slope (2): Indicates that for each additional inch in height, the person’s weight increases by 2 pounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a model’s cost function. It works by iteratively adjusting the model parameters in the direction that reduces the cost, using gradients to determine the direction of steepest descent. The algorithm starts with initial parameter values, calculates gradients, updates parameters using a step size called the learning rate, and repeats the process until the cost function converges to its minimum.\n",
    "\n",
    "In machine learning, gradient descent is essential for training models by minimizing the error or loss function. It is used in algorithms like linear regression, logistic regression, and neural networks to update model parameters and improve prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that models the relationship between one dependent variable and two or more independent variables. The model estimates the dependent variable based on multiple predictors, incorporating their combined effects.\n",
    "\n",
    "Formula: Y = a + b1X1 + b2X2 + ... + bn*Xn\n",
    "\n",
    "\n",
    "Difference from Simple Linear Regression\n",
    "\n",
    "Simple linear regression involves only one independent variable and one dependent variable, representing their relationship with a single line. The model formula is Y = a + b*X + e, where X is the single predictor. In contrast, multiple linear regression includes multiple independent variables, allowing for a more complex and accurate model that accounts for various factors influencing the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, making it difficult to assess their individual effects on the dependent variable. This can lead to unstable coefficient estimates and unreliable statistical tests.\n",
    "\n",
    "Detecting Multicollinearity\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate VIF values; a VIF greater than 10 indicates multicollinearity.\n",
    "\n",
    "Correlation Matrix: Examine the correlation matrix for high correlations between independent variables.\n",
    "\n",
    "Addressing Multicollinearity\n",
    "\n",
    "Remove Variables: Eliminate highly correlated independent variables.\n",
    "\n",
    "Combine Variables: Create a composite variable from correlated predictors.\n",
    "\n",
    "Principal Component Analysis (PCA): Transform correlated variables into uncorrelated components.\n",
    "\n",
    "Regularization: Use Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Polynomial regression models the relationship between the independent variable and the dependent variable using a polynomial function. This approach allows for capturing non-linear relationships by including higher-degree terms of the independent variable in the model.\n",
    "\n",
    "Formula: Y = a + b1X + b2X^2 + b3X^3 + ... + bnX^n + e\n",
    "\n",
    "Y: Dependent variable;\n",
    "X: Independent variable;\n",
    "a: Intercept;\n",
    "b1, b2, ..., bn: Coefficients for each polynomial term;\n",
    "e: Error term\n",
    "\n",
    "Difference from Linear Regression\n",
    "\n",
    "Linear Regression: Models the relationship as a straight line with the formula Y = a + b*X + e, representing a linear relationship.\n",
    "\n",
    "Polynomial Regression: Uses polynomial terms (e.g., X^2, X^3) to model non-linear relationships, allowing for more complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Advantages of Polynomial Regression\n",
    "\n",
    "Polynomial regression can capture complex, non-linear relationships between the independent and dependent variables, which linear regression cannot model. By adding polynomial terms, the model can fit curves and more intricate patterns in the data, making it suitable for scenarios where the relationship between variables is not strictly linear. This flexibility allows polynomial regression to improve model accuracy and provide better predictions in cases where linear assumptions do not hold.\n",
    "\n",
    "Disadvantages of Polynomial Regression\n",
    "\n",
    "Polynomial regression can become overfitted when using high-degree polynomials, leading to a model that fits the training data very closely but performs poorly on new, unseen data. The model’s complexity increases with higher-degree terms, which can make it more difficult to interpret and computationally intensive. Additionally, polynomial regression can introduce multicollinearity among polynomial terms, affecting the stability of coefficient estimates.\n",
    "\n",
    "When to Use Polynomial Regression\n",
    "\n",
    "Polynomial regression is preferable when you have a reason to believe that the relationship between variables is non-linear and when you observe patterns in the data that suggest curvature or complex relationships. It is useful in situations where linear models fail to capture the underlying trends or where visual inspection and exploratory data analysis indicate non-linear patterns. However, it is important to balance model complexity with the risk of overfitting and ensure that polynomial terms are chosen appropriately based on the data and problem context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
