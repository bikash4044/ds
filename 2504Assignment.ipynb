{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Eigenvalues and eigenvectors are fundamental in linear algebra, especially for matrix analysis and dimensionality reduction techniques like PCA (Principal Component Analysis).\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "Definition: Eigenvalues are scalars that describe how much the eigenvectors are scaled during a linear transformation.\n",
    "Role: They represent the amount of stretching or shrinking applied to the eigenvectors by the matrix transformation.\n",
    "Eigenvectors:\n",
    "\n",
    "Definition: Eigenvectors are vectors that remain in the same direction after a linear transformation but are scaled by the eigenvalue.\n",
    "Role: They provide the direction along which the transformation acts.\n",
    "Eigen-Decomposition:\n",
    "\n",
    "Definition: Eigen-decomposition is the process of breaking down a matrix into its eigenvalues and eigenvectors.\n",
    "Matrix Representation: For a matrix A, it can be decomposed as \n",
    "\n",
    "A=VΛV ^ −1\n",
    " , where:\n",
    "V is the matrix of eigenvectors.\n",
    "Λ is the diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[5. 2.]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.70710678 -0.4472136 ]\n",
      " [ 0.70710678  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 1],\n",
    "              [2, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Eigen Decomposition breaks down a square matrix into:\n",
    "Eigenvalues: Numbers that show how much the matrix stretches or shrinks vectors.\n",
    "Eigenvectors: Directions that remain unchanged in direction after applying the matrix.\n",
    "\n",
    "Significance of Eigen Decomposition\n",
    "\n",
    "Simplifies Calculations:\n",
    "\n",
    "Makes matrix operations like powers and inversions easier.\n",
    "\n",
    "Helps with Analysis:\n",
    "\n",
    "Reveals properties and behavior of the matrix.\n",
    "\n",
    "Useful in Data Science:\n",
    "\n",
    "PCA: Helps reduce data dimensions.\n",
    "Matrix Factorization: Used in recommendations and compression.\n",
    "\n",
    "Understanding Transformations:\n",
    "\n",
    "Shows how matrices affect vectors.\n",
    "\n",
    "Solving Equations:\n",
    "\n",
    "Simplifies complex systems of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "\n",
    "Ans:\n",
    "\n",
    "A square matrix A is diagonalizable if and only if it has n linearly independent eigenvectors, where n is the size of the matrix (i.e., A is an n x n matrix).\n",
    "\n",
    "Brief Proof\n",
    "Diagonalizable Matrix Definition:\n",
    "\n",
    "A matrix A is diagonalizable if there exists a matrix V and a diagonal matrix Lambda such that A = V * Lambda * V^(-1). Here, V contains the eigenvectors of A, and Lambda contains the eigenvalues.\n",
    "\n",
    "Requirement for Diagonalization:\n",
    "\n",
    "To express A as V * Lambda * V^(-1), matrix V must be invertible. For V to be invertible, its columns (which are the eigenvectors of A) must be linearly independent.\n",
    "\n",
    "Number of Eigenvectors:\n",
    "\n",
    "A matrix A can be diagonalized if and only if there are n linearly independent eigenvectors. If A has n distinct eigenvalues, it always has n linearly independent eigenvectors. If A has fewer than n linearly independent eigenvectors, it cannot be diagonalized.\n",
    "\n",
    "Proof Sketch:\n",
    "\n",
    "Sufficient Condition: If A has n linearly independent eigenvectors, then V, the matrix of these eigenvectors, is invertible. Therefore, A = V * Lambda * V^(-1), showing that A is diagonalizable.\n",
    "\n",
    "Necessary Condition: If A is diagonalizable, then there must be n linearly independent eigenvectors. If there are fewer than n eigenvectors, V would not be invertible, and A cannot be diagonalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "Ans:\n",
    "\n",
    "The Spectral Theorem is a fundamental result in linear algebra that plays a crucial role in the context of eigen-decomposition. Here’s how it relates to eigen-decomposition and the diagonalizability of matrices:\n",
    "\n",
    "Spectral Theorem Statement:\n",
    "\n",
    "The Spectral Theorem states that any symmetric matrix (or Hermitian matrix in the complex case) can be diagonalized by an orthogonal (or unitary) matrix. Specifically, for a symmetric matrix A:\n",
    "A = V * Lambda * V^T\n",
    "V is an orthogonal matrix (its inverse is its transpose).\n",
    "Lambda is a diagonal matrix with the eigenvalues of A.\n",
    "Significance:\n",
    "\n",
    "Diagonalizability: The theorem guarantees that symmetric matrices are always diagonalizable. This means that for symmetric matrices, eigen-decomposition is always possible.\n",
    "Orthogonality: The eigenvectors of a symmetric matrix can be chosen to be orthonormal. This simplifies many computations and ensures numerical stability.\n",
    "Real Eigenvalues: The theorem ensures that the eigenvalues of a symmetric matrix are real, which is important for many practical applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[2.38196601 4.61803399]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.52573111 -0.85065081]\n",
      " [-0.85065081 -0.52573111]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[5. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 1],\n",
    "              [2, 3]])\n",
    "\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of Eigenvalues\n",
    "\n",
    "Scaling Factor: Eigenvalues show how much the corresponding eigenvectors are stretched or shrunk.\n",
    "Matrix Properties: They provide insights into the matrix’s behavior and stability.\n",
    "Principal Components: In PCA, eigenvalues help determine the variance captured by each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Eigenvectors are vectors that, when multiplied by a matrix, only get scaled and not changed in direction.\n",
    "\n",
    "Eigenvalues are the scalars that represent how much the eigenvectors are scaled.\n",
    "\n",
    "Relationship: For a matrix A, if v is an eigenvector and lambda is the eigenvalue, then\n",
    "\n",
    "A * v = lambda * v\n",
    "\n",
    "Here, v is scaled by lambda but its direction stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Eigenvectors:\n",
    "\n",
    "Direction: Eigenvectors are special vectors that, when a matrix transformation is applied, their direction remains unchanged. They only get scaled, not rotated.\n",
    "Visual: Imagine stretching or compressing along certain directions without changing the direction itself. These directions are given by the eigenvectors.\n",
    "Eigenvalues:\n",
    "\n",
    "Scaling Factor: Eigenvalues are the factors by which the eigenvectors are stretched or compressed.\n",
    "Visual: If an eigenvector is stretched by a factor of lambda, then the eigenvalue is lambda, which indicates how much the vector length increases or decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Real-World Applications of Eigen Decomposition:\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "Use: Dimensionality reduction and feature extraction.\n",
    "How: Eigen decomposition of the covariance matrix identifies principal components that capture the most variance in the data.\n",
    "\n",
    "Image Compression:\n",
    "\n",
    "Use: Reducing the size of image files.\n",
    "How: Eigen decomposition in Singular Value Decomposition (SVD) helps compress images by approximating them with fewer components.\n",
    "\n",
    "Stability Analysis in Systems:\n",
    "\n",
    "Use: Assessing the stability of dynamic systems.\n",
    "How: Eigenvalues of system matrices indicate whether system responses grow or decay over time.\n",
    "\n",
    "Google’s PageRank Algorithm:\n",
    "\n",
    "Use: Ranking web pages.\n",
    "How: Eigen decomposition of the link matrix helps compute the importance of web pages.\n",
    "\n",
    "Facial Recognition:\n",
    "\n",
    "Use: Identifying and verifying faces.\n",
    "How: Eigenfaces, derived from eigen decomposition, are used to represent and recognize facial features.\n",
    "\n",
    "Structural Analysis:\n",
    "\n",
    "Use: Analyzing vibrations and stresses in structures.\n",
    "How: Eigenvalues represent natural frequencies, and eigenvectors represent mode shapes in structural engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "A matrix cannot have more than one set of eigenvalues, but it can have multiple eigenvectors corresponding to each eigenvalue. Here’s a simple explanation:\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "Uniqueness: Each eigenvalue is unique to the matrix, and there is only one set of eigenvalues for a given matrix. The eigenvalues are determined by solving the characteristic polynomial.\n",
    "\n",
    "Eigenvectors:\n",
    "\n",
    "Multiplicity: Each eigenvalue can have multiple eigenvectors. These eigenvectors form a basis for the eigenspace associated with that eigenvalue.\n",
    "Scaling and Basis: If v is an eigenvector for eigenvalue lambda, then any scalar multiple of v is also an eigenvector for lambda. Moreover, the eigenspace for a particular eigenvalue can be spanned by multiple linearly independent eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
    "\n",
    "Ans:\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "Use: Dimensionality reduction and feature extraction.\n",
    "How: PCA uses eigen-decomposition of the covariance matrix of the data to identify principal components. These components are the directions in which the data varies the most. By projecting the data onto these components, PCA reduces the number of dimensions while preserving as much variance as possible.\n",
    "\n",
    "Singular Value Decomposition (SVD):\n",
    "\n",
    "Use: Matrix factorization and data compression.\n",
    "How: SVD is a generalization of eigen-decomposition for non-square matrices. It decomposes a matrix into three other matrices: U, S, and V. In applications like image compression, SVD is used to approximate a matrix with fewer dimensions, preserving important information while reducing storage requirements.\n",
    "\n",
    "Latent Semantic Analysis (LSA):\n",
    "\n",
    "Use: Text data analysis and topic modeling.\n",
    "How: LSA uses SVD to decompose the term-document matrix of a corpus. This decomposition helps to identify the latent structure in the data, revealing underlying topics or concepts. By reducing the dimensionality, LSA can capture semantic relationships between words and documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
